@article{baker2007middlebury,
  author={Baker, Simon and Roth, Stefan and Scharstein, Daniel and Black, Michael J. and Lewis, J.P. and Szeliski, Richard},
  journal={2007 IEEE 11th International Conference on Computer Vision}, 
  title={A Database and Evaluation Methodology for Optical Flow}, 
  year={2007},
  pages={1-8},
  doi={10.1109/ICCV.2007.4408903}
}
@article{hornschunck1981,
  title = {Determining optical flow},
  journal = {Artificial Intelligence},
  volume = {17},
  number = {1},
  pages = {185-203},
  year = {1981},
  issn = {0004-3702},
  doi = {https://doi.org/10.1016/0004-3702(81)90024-2},
  url = {https://www.sciencedirect.com/science/article/pii/0004370281900242},
  author = {Berthold K.P. Horn and Brian G. Schunck},
  abstract = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.}
}
@article{huang2022flowformer,
  title={FlowFormer: A Transformer Architecture for Optical Flow},
  author={Huang, Zhaoyang and Shi, Xiaoyu and Zhang, Chao and Wang, Qiang and Cheung, Ka Chun and Qin, Hongwei and Dai, Jifeng and Li, Hongsheng},
  journal={{ECCV}},
  year={2022}
}

@inproceedings{shi2023flowformerplusplus,
  title={Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation},
  author={Shi, Xiaoyu and Huang, Zhaoyang and Li, Dasong and Zhang, Manyuan and Cheung, Ka Chun and See, Simon and Qin, Hongwei and Dai, Jifeng and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1599--1610},
  year={2023}
}
@inproceedings{Zachary2020RAFT,
  author = {Teed, Zachary and Deng, Jia},
  title = {RAFT: Recurrent All-Pairs Field Transforms for Optical Flow},
  year = {2020},
  isbn = {978-3-030-58535-8},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  url = {https://doi.org/10.1007/978-3-030-58536-5_24},
  doi = {10.1007/978-3-030-58536-5_24},
  abstract = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\%, a 16\% error reduction from the best published result (6.10\%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at .},
  booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II},
  pages = {402–419},
  numpages = {18},
  location = {Glasgow, United Kingdom}
}
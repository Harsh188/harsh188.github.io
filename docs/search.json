[
  {
    "objectID": "experience/experience.html",
    "href": "experience/experience.html",
    "title": "Experience",
    "section": "",
    "text": "Intel\n\n\n\nComputer Vision\n\nAutonomous Driving\n\nDeep Learning\n\nResearch\n\n\n\nDesigned novel semi-supervised learning framework for advanced driver-assistance systems (ADAS) to automate object detection labeling. Resulted in 20% improvement in‚Ä¶\n\n\n\n\n\nJan, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Summer of Code | Red Hen Labs\n\n\n\nDeep Learning\n\nComputer Vision\n\nHPC\n\n\n\nAutomated segmentation of old TV Broadcast recordings using multi-modal deep learning. Enabled 15x efficiency through multi-threading and array jobs on a largescale HPC GPU‚Ä¶\n\n\n\n\n\nJun, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nPES University\n\n\n\nComputer Vision\n\nData Analysis\n\n\n\nCreated course slides, practical python workbooks and automated labs for the Data Analytics and Image Processing and Computer Vision courses. Improved understanding of‚Ä¶\n\n\n\n\n\nJun, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndian Institute of Science\n\n\n\nWiFi Networks\n\nResearch\n\nStatistical Modeling\n\n\n\nResearched Markov Chain-based stochastic models to reduce 802.11 WiFi router packet collisions, enhancing throughput. Verified results through NetSim and QualNet simulations.\n\n\n\n\n\nMay, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnrole Inc.\n\n\n\nData Analysis\n\nFlutter\n\nAPI\n\n\n\nDesigned novel semi-supervised learning framework for advanced driver-assistance systems (ADAS) to automate object detection labeling. Resulted in 20% improvement in‚Ä¶\n\n\n\n\n\nJun, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experience/experience.html#work",
    "href": "experience/experience.html#work",
    "title": "Experience",
    "section": "",
    "text": "Intel\n\n\n\nComputer Vision\n\nAutonomous Driving\n\nDeep Learning\n\nResearch\n\n\n\nDesigned novel semi-supervised learning framework for advanced driver-assistance systems (ADAS) to automate object detection labeling. Resulted in 20% improvement in‚Ä¶\n\n\n\n\n\nJan, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Summer of Code | Red Hen Labs\n\n\n\nDeep Learning\n\nComputer Vision\n\nHPC\n\n\n\nAutomated segmentation of old TV Broadcast recordings using multi-modal deep learning. Enabled 15x efficiency through multi-threading and array jobs on a largescale HPC GPU‚Ä¶\n\n\n\n\n\nJun, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nPES University\n\n\n\nComputer Vision\n\nData Analysis\n\n\n\nCreated course slides, practical python workbooks and automated labs for the Data Analytics and Image Processing and Computer Vision courses. Improved understanding of‚Ä¶\n\n\n\n\n\nJun, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndian Institute of Science\n\n\n\nWiFi Networks\n\nResearch\n\nStatistical Modeling\n\n\n\nResearched Markov Chain-based stochastic models to reduce 802.11 WiFi router packet collisions, enhancing throughput. Verified results through NetSim and QualNet simulations.\n\n\n\n\n\nMay, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnrole Inc.\n\n\n\nData Analysis\n\nFlutter\n\nAPI\n\n\n\nDesigned novel semi-supervised learning framework for advanced driver-assistance systems (ADAS) to automate object detection labeling. Resulted in 20% improvement in‚Ä¶\n\n\n\n\n\nJun, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experience/experience.html#volunteering",
    "href": "experience/experience.html#volunteering",
    "title": "Experience",
    "section": "Volunteering",
    "text": "Volunteering\n\n\n\n\n\n\n\n\n\n\nOrganizer\n\n\nTensorFlow User Group Bangalore\n\n\n\nManagement\n\n\n\nManaged event communication and sponsorship, facilitating valuable industry connections.\n\n\n\n\n\nJun, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSubject Matter Expert\n\n\nPES University\n\n\n\nDeep Learning\n\nMachine Learning\n\nNeural Networks\n\n\n\nDeveloped and taught four-week curriculum for the course ‚ÄòIntroduction to Machine Learning‚Äô and held a two-day workshop on ‚ÄòNeural Networks from Scratch‚Äô for 100‚Ä¶\n\n\n\n\n\nMay, 2021\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experience/experience.html#projects",
    "href": "experience/experience.html#projects",
    "title": "Experience",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\nAbstractive Text Summarized using PEGASUS\n\n\n\nMachine Learning\n\nArtificial Intelligence\n\nGCP\n\n\n\nThis project is a fully scalable end-to-end abstractive summarizer built on the state of the art PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive‚Ä¶\n\n\n\n\n\nJan, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis\n\n\n\nNLP\n\nMachine Learning\n\nArtificial Intelligence\n\n\n\nThis is a small NLP project which uses sentiment analysis and machine learning to classify words into positive or negative connotations. This project is a part of the final‚Ä¶\n\n\n\n\n\nMay, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nChatBot using DialogFlow\n\n\n\nNLP\n\nMachine Learning\n\nGCP\n\n\n\nBy utilizing google‚Äôs DialogFlow, I built a simple chatbot system that can detect intents and reply based on the category identified. I integrated this chatbot to Facebook‚Ä¶\n\n\n\n\n\nJan, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "For an updated list of publications and citations, check out my Google Scholar.\nFusing Pseudo Labels with Weak Supervision for Dynamic Traffic Scenarios\nHarshith Mohan Kumar, Sean Lawrence\nICCV 2023 BRAVO Workshop\npaper website\nOCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios\nAditya N Ganesh, Dhruval Pobbathi Badrinath, Harshith Mohan Kumar, Priya S S, Surabhi Narayan\nCVPR 2023 T4V Workshop\npaper website\nGraphCoReg: Co-Training for Regression on Temporal Graphs\nHarshith Mohan Kumar, Vishruth Veerendranath, Vibha Masti, Divya Shekar, Bhaskarjyoti Das\nECML-PKDD 2022 MLG Workshop\nüèÜ Best Student Paper Award\npaper\nMultivariate Covid-19 Forecasting with Vaccinations as a Factor\nVibha Masti, Vishruth Veerendranath, Harshith Mohan Kumar\nIEEE TENSYMP 2022, also presented at ICML 2022 Healthcare-AI Workshop\npaper\nSemi-supervised Learning with In-domain Pre-training and Deep Co-training\nBhaskarjyoti Das, Harshith Mohan Kumar, Divya Shekar, Mohammed Zayd Jamadar\nSpringer ICICCT 2022\npaper\nClassification of Recyclable Waste Generated in Indian Households\nHarshith MohanKumar, Mohammed Zayd Jamadar, S Natarajan, H Nithin\nSpringer WorldS4 2021\npaper"
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "2023",
    "text": "2023\n\nFusing Pseudo Labels with Weak Supervision for Dynamic Traffic Scenarios \n\nHarshith Mohan Kumar, Sean Lawrence\nICCV 2023 BRAVO Workshop | Website\n\nOCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios\n[Paper] [Additional]\n\nAditya N Ganesh, Dhruval Pobbathi Badrinath, Harshith Mohan Kumar, Priya S S, Surabhi Narayan\nCVPR 2023 T4V Workshop | Website"
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "2022",
    "text": "2022\n\nGraphCoReg: Co-Training for Regression on Temporal Graphs\n[Paper]\n\nHarshith Mohan Kumar, Vishruth Veerendranath, Vibha Masti, Divya Shekar, Bhaskarjyoti Das\nECML-PKDD 2022 International Workshop on Mining and Learning with Graphs | Submissions\nBest Student Paper Award\n\nMultivariate Covid-19 Forecasting with Vaccinations as a factor: the case of India and USA\n[Paper]\n\nVibha Masti, Vishruth Veerendranath, Harshith Mohan Kumar\nICML 2022 1st Workshop on Healthcare AI and COVID-19\nIEEE Region 10 Symposium (TENSYMP) 2022\n\nSemi-supervised Learning with In-domain Pre-training and Deep Co-training\n[Paper]\n\nBhaskarjyoti Das, Harshith Mohan Kumar, Divya Shekar, Mohammed Zayd Jamadar\nSpringer Nature ICICCT 2022"
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications",
    "section": "2021",
    "text": "2021\n\nClassification of Recyclable Waste Generated in Indian Households\n[Paper]\n\nHarshith MohanKumar, Mohammed Zayd Jamadar, S Natarajan, H Nithin\nSpringer WorldS4 2021"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Harshith Mohan Kumar",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \n\n\nI‚Äôm a Machine Learning Engineer specializing in computer vision and computational imaging. As a Research Assistant at UC Riverside, I develop physics-inspired deep models, pushing boundaries in perception systems.\nI hold an MS from UC Riverside, where I researched thermal optical flow under the guidance of Dr.¬†Vishwanath Saragadam and Dr.¬†Amit Roy-Chowdhury, and a BS from PES University.\nPreviously at Intel, I developed a severity ranking system for ADAS alerts through scene understanding. I engineered a large-scale video analysis pipeline during Google Summer of Code and worked as a research fellow at IISc under Dr.¬†Anurag Kumar.\n\n\nSeeking full-time ML/CV roles to build production-ready models for real-world applications‚Äîlet‚Äôs discuss how I can contribute to your team. Lets Connect!"
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Harshith Mohan Kumar",
    "section": "",
    "text": "Harshith is a Machine Learning Engineer specializing in computer vision, deep learning and\nI‚Äôm Harshith, welcome to my personal site!\nI‚Äôm currently a Graduate Student Researcher at UC Riverside, where I‚Äôm passionately pursuing my Masters in Computer Science. My research interests encompass a diverse range of topics in the field of computer vision, computational photography and deep learning.\nWith the privilege of interning at Intel, contributing to Google Summer of Code, and being a research fellow at the Indian Institute of Science, I bring a rich tapestry of experiences.\nMy journey has been punctuated by multiple acceptances at top conferences, showcasing my commitment to excellence. Feel free to explore my research publications here."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Harshith Mohan Kumar",
    "section": "",
    "text": "MSCS | Sept 2023 - Present\n\n\n\n\nB.Tech CS | Aug 2019 - June 2023"
  },
  {
    "objectID": "index.html#recent-news",
    "href": "index.html#recent-news",
    "title": "Harshith Mohan Kumar",
    "section": "Recent News",
    "text": "Recent News\n\n2025\n\n[04/19/25] - Defended my MS Thesis on Thermal Optical Flow\n\n\n\n2024\n\n[06/01/24] - Working as Graduate Student Researcher\n[02/06/24] - Featured on podcast episode\n[01/01/24] - Joined the Computational Optics Lab, advised by Prof.¬†Vishwanath Saragadam\n\n\n\n2023\n\n[08/07/23] - Extended abstract accepted at ICCV‚Äô23 BRAVO Workshop\n[07/20/23] - OCTraN now on ArXiv\n[05/29/23] - OCTraN paper accepted at CVPR‚Äô23 T4V Workshop\n[01/02/23] - Working at at Intel\n\n\n\n2022\n\n[09/23/22] - Won Best Student Paper Award at ECML-PKDD‚Äô22 MLG workshop"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "fun.html",
    "href": "fun.html",
    "title": "Fun",
    "section": "",
    "text": "Looks like I‚Äôm not having too much fun right now!"
  },
  {
    "objectID": "anki.html",
    "href": "anki.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#hi-im-harsh",
    "href": "index.html#hi-im-harsh",
    "title": "Harshith Mohan Kumar",
    "section": "",
    "text": "I‚Äôm a Machine Learning Engineer specializing in computer vision and computational imaging. As a Research Assistant at UC Riverside, I develop physics-inspired deep models, pushing boundaries in perception systems.\nI hold an MS from UC Riverside, where I researched thermal optical flow under the guidance of Dr.¬†Vishwanath Saragadam and Dr.¬†Amit Roy-Chowdhury, and a BS from PES University.\nPreviously at Intel, I developed a severity ranking system for ADAS alerts through scene understanding. I engineered a large-scale video analysis pipeline during Google Summer of Code and worked as a research fellow at IISc under Dr.¬†Anurag Kumar.\n\n\nSeeking full-time ML/CV roles to build production-ready models for real-world applications‚Äîlet‚Äôs discuss how I can contribute to your team. Lets Connect!"
  },
  {
    "objectID": "blog/2025-04-19/index.html",
    "href": "blog/2025-04-19/index.html",
    "title": "Visual guide to Optical Flow üåä",
    "section": "",
    "text": "Have you wondered how your optical üñ±Ô∏è mouse tracks movement? It uses a tiny üì∑ camera to capture surface images at high speed, then computes optical flow‚Äîthe motion of pixels between frames‚Äîto detect direction and speed. But optical flow isn‚Äôt just for mice; it powers everything from stabilization, self-driving cars üöó and even tracking your favorite sports player ‚öΩ in real time.\nWhat is optical flow?\n\nOptical flow is a 2D vector field of apparent velocities which describes the movement of brightness patterns in an image\n\nBerthold K.P. Horn and his Ph.D.¬†student Brian G. Schunck laid the mathematical foundation for motion estimation in their 1981 paper, ‚ÄúDetermining Optical Flow‚Äù (Horn and Schunck 1981) which became one of the most influential works in the field of computer vision.\nOptical flow continues to remain a core problem in computer vision. In this post, I‚Äôll break it down: what it is, how it works mathematically, and the latest advances like RAFT and FlowFormer++. Finally, I‚Äôll wrap it up by demonstrating real-world applications and discuss unsolved challenges.\nUse the Table of Contents (top-right) or the Quick Navigation below to jump to sections.\n\n\n\n\n\n\nQuick Navigation (Click to Expand)\n\n\n\n\n\n\nMain Components of Optical Flow\n\nThe Math Behind It\nClassical Solutions\nModern Advances\n\nApplications\n\nChallenges"
  },
  {
    "objectID": "blog/2025-04-19/index.html#introduction",
    "href": "blog/2025-04-19/index.html#introduction",
    "title": "Visual guide to Optical Flow üåä",
    "section": "",
    "text": "Have you wondered how your optical üñ±Ô∏è mouse tracks movement? It uses a tiny üì∑ camera to capture surface images at high speed, then computes optical flow‚Äîthe motion of pixels between frames‚Äîto detect direction and speed. But optical flow isn‚Äôt just for mice; it powers everything from stabilization, self-driving cars üöó and even tracking your favorite sports player ‚öΩ in real time.\nWhat is optical flow?\n\nOptical flow is a 2D vector field of apparent velocities which describes the movement of brightness patterns in an image\n\nBerthold K.P. Horn and his Ph.D.¬†student Brian G. Schunck laid the mathematical foundation for motion estimation in their 1981 paper, ‚ÄúDetermining Optical Flow‚Äù (Horn and Schunck 1981) which became one of the most influential works in the field of computer vision.\nOptical flow continues to remain a core problem in computer vision. In this post, I‚Äôll break it down: what it is, how it works mathematically, and the latest advances like RAFT and FlowFormer++. Finally, I‚Äôll wrap it up by demonstrating real-world applications and discuss unsolved challenges.\nUse the Table of Contents (top-right) or the Quick Navigation below to jump to sections.\n\n\n\n\n\n\nQuick Navigation (Click to Expand)\n\n\n\n\n\n\nMain Components of Optical Flow\n\nThe Math Behind It\nClassical Solutions\nModern Advances\n\nApplications\n\nChallenges"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog! Here I write about computer vision, machine learning, and life lessons.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisual guide to Optical Flow üåä\n\n\nProbably everything you need to know about optical flow üòä\n\n\n\nHarshith Mohan Kumar\n\n\nApr 19, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2025-04-19/index.html#components",
    "href": "blog/2025-04-19/index.html#components",
    "title": "Guide to Optical Flow üåä",
    "section": "Components",
    "text": "Components\n\nVisualization\n\n# Code cells work too!\nimport pandas as pd\n\n\n\nWarping\n\n\nMetrics"
  },
  {
    "objectID": "blog/2025-04-19/index.html#the-math",
    "href": "blog/2025-04-19/index.html#the-math",
    "title": "Guide to Optical Flow üåä",
    "section": "The Math",
    "text": "The Math"
  },
  {
    "objectID": "blog/2025-04-19/index.html#classical-approach",
    "href": "blog/2025-04-19/index.html#classical-approach",
    "title": "Visual guide to Optical Flow üåä",
    "section": "Classical Approach",
    "text": "Classical Approach\nThere are two implementations of classical optical flow with OpenCV:\n\nLucas-Kanade (Sparse)\nKey Idea:\n\nTracks feature points (corners/edges) by assuming constant flow in local neighborhoods (typically 3√ó3 or 5√ó5 windows)\nSolves the optical flow equation \\(I_x u + I_y v + I_t = 0\\) using least squares\n\n\n\n\nLucas-Kanade optical flow visualization showing tracked feature points and their motion vectors\n\n\n\nImplementation\nflow = cv2.calcOpticalFlowPyrLK(prev_frame, next_frame, \n    prev_pts, None, winSize=(7,7))\n\n\nCharacteristics\n\n\nPros:\n‚úÖ Fast (only computes for keypoints)\n‚úÖ Works well for small displacements\n\n\nCons:\n‚ùå Fails in textureless regions\n‚ùå Struggles with large motions\n\n\n\n\n\n\nFarneb√§ck (Dense)\nKey Idea:\n\nModels image patches as quadratic polynomials to estimate flow at every pixel\nUses polynomial expansion to approximate motion\n\n\n\n\nFarneb√§ck optical flow visualization using HSV color coding (hue represents direction, saturation represents magnitude)\n\n\n\n\n\nFarneb√§ck optical flow visualization with motion vectors overlaid on the original frame\n\n\n\nImplementation\nflow = cv2.calcOpticalFlowFarneback(prev_frame, next_frame, \n    None, pyr_scale=0.5, levels=3, winsize=7)\n\n\nCharacteristics\n\n\nPros:\n‚úÖ Full motion field\n‚úÖ Handles moderate occlusion\n\n\nCons:\n‚ùå Computationally heavy\n‚ùå Blurry motion boundaries"
  },
  {
    "objectID": "blog/2025-04-19/index.html#sota",
    "href": "blog/2025-04-19/index.html#sota",
    "title": "Visual guide to Optical Flow üåä",
    "section": "SOTA",
    "text": "SOTA\n\nRAFT Recurrent All-Pairs Field Transforms\nBuilds dense 4D all-pairs correlation volumes by matching every pixel with every other and employs a lightweight recurrent GRU-style updater to iteratively refine the flow field (Teed and Deng 2020).\n\n\n\n\nRAFT architecture showing the correlation volume computation and iterative updates\n\n\n\n\n\nFlowFormer++\nImproves optical flow estimation by introducing a hierarchical Transformer-based architecture with cost-volume encoding and decoupled update modules, enabling long-range and fine-grained motion modeling (Shi et al. 2023).\n\n\n\n\nFlowFormer++ architecture showing the transformer-based matching and cost volume encoding"
  },
  {
    "objectID": "blog/2025-04-19/index.html#metrics-1",
    "href": "blog/2025-04-19/index.html#metrics-1",
    "title": "Guide to Optical Flow üåä",
    "section": "Metrics",
    "text": "Metrics"
  },
  {
    "objectID": "blog/2025-04-19/index.html#applications",
    "href": "blog/2025-04-19/index.html#applications",
    "title": "Visual guide to Optical Flow üåä",
    "section": "Applications",
    "text": "Applications\n\n\n\n\n\n\n\nApplication\nDescription\n\n\n\n\nVideo Synthesis\nGenerates intermediate frames by estimating motion between video frames, enabling smooth transitions like slow-motion.\n\n\nVideo Inpainting\nFills in missing or damaged video regions by tracking the motion of surrounding pixels, ensuring temporal consistency.\n\n\nVideo Stabilization\nCompensates for camera shake by estimating motion and stabilizing consecutive frames, creating smoother video output.\n\n\nLow Level Vision\nProvides motion information for tasks like object tracking, motion detection, and scene reconstruction.\n\n\nStereo and SLAM\nUsed in depth estimation and 3D reconstruction by combining with stereo images, crucial for robotics and self-driving."
  },
  {
    "objectID": "blog/2025-04-19/index.html#beauty-of-the-math",
    "href": "blog/2025-04-19/index.html#beauty-of-the-math",
    "title": "Guide to Optical Flow üåä",
    "section": "Beauty of the Math",
    "text": "Beauty of the Math"
  },
  {
    "objectID": "blog/2025-04-19/index.html#mathematical-beauty",
    "href": "blog/2025-04-19/index.html#mathematical-beauty",
    "title": "Guide to Optical Flow üåä",
    "section": "Mathematical Beauty",
    "text": "Mathematical Beauty\nThe optical flow problem aims to estimate the motion of pixels between two consecutive frames. The foundational equation of optical flow is based on the brightness constancy assumption, which posits that the intensity of a pixel does not change between consecutive frames. This assumption leads to the well-known optical flow constraint equation:\n\\[\nI_x u + I_y v + I_t = 0\n\\]\nWhere:\n\n\\(I_x\\) and \\(I_y\\) are the partial derivatives of the image intensity with respect to the horizontal (\\(x\\)) and vertical (\\(y\\)) spatial dimensions.\n\\(I_t\\) is the partial derivative of the image intensity with respect to time (\\(t\\)).\n\\(u\\) and \\(v\\) are the components of the optical flow vector, representing the horizontal and vertical motion of a pixel.\n\nThe equation above implies that for each pixel, the rate of change of intensity over time can be attributed to the motion of the pixel in the image plane. This is the optical flow constraint.\n\n\nHorn and Schunck‚Äôs Optical Flow Model\n(Horn and Schunck 1981) introduced a regularized version of the optical flow problem to address the ill-posedness of the optical flow equation, as the optical flow constraint alone does not provide a unique solution. Their model adds a smoothness assumption, which assumes that neighboring pixels have similar motion.\nThe formulation they proposed can be written as:\n\\[\n\\min_{u,v} \\int \\left[ (I_x u + I_y v + I_t)^2 + \\alpha^2 (I_x^2 + I_y^2)(u_x^2 + u_y^2 + v_x^2 + v_y^2) \\right] dx dy\n\\]\nWhere:\n\nThe first term is the data term, which enforces the optical flow constraint.\nThe second term is the smoothness term, which enforces spatial smoothness of the flow field. It penalizes large variations in the flow between neighboring pixels.\n\\(\\alpha\\) is a regularization parameter that controls the trade-off between the data term and the smoothness term.\n\nHere, \\(u_x\\) and \\(u_y\\) represent the spatial derivatives of \\(u\\) (the horizontal flow component), and \\(v_x\\) and \\(v_y\\) represent the spatial derivatives of \\(v\\) (the vertical flow component). The smoothness term ensures that the flow field does not have sharp discontinuities, which helps in obtaining a physically plausible solution.\n\n\n\nKey Assumptions of the Horn-Schunck Method:\n\nBrightness Constancy: The pixel intensity does not change over time for any given object in the scene.\nSmoothness: The flow is smooth across the image, meaning that neighboring pixels move in similar ways.\nGlobal Regularization: The optical flow is computed globally across the entire image, meaning that all pixels are influenced by the flow of neighboring pixels through the regularization term.\n\n\nThis model led to a well-known global method for optical flow computation, which involves solving the system of equations derived from the above functional using iterative techniques, such as gradient descent or other optimization algorithms."
  },
  {
    "objectID": "blog/2025-04-19/index.html#optimizations",
    "href": "blog/2025-04-19/index.html#optimizations",
    "title": "Guide to Optical Flow üåä",
    "section": "Optimizations",
    "text": "Optimizations\n\nCoarse-to-Fine"
  },
  {
    "objectID": "blog/2025-04-19/index.html#references",
    "href": "blog/2025-04-19/index.html#references",
    "title": "Visual guide to Optical Flow üåä",
    "section": "References",
    "text": "References\n\n\nBaker, Simon, Stefan Roth, Daniel Scharstein, Michael J. Black, J. P. Lewis, and Richard Szeliski. 2007. ‚ÄúA Database and Evaluation Methodology for Optical Flow.‚Äù 2007 IEEE 11th International Conference on Computer Vision, 1‚Äì8. https://doi.org/10.1109/ICCV.2007.4408903.\n\n\nHorn, Berthold K. P., and Brian G. Schunck. 1981. ‚ÄúDetermining Optical Flow.‚Äù Artificial Intelligence 17 (1): 185‚Äì203. https://doi.org/https://doi.org/10.1016/0004-3702(81)90024-2.\n\n\nShi, Xiaoyu, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. 2023. ‚ÄúFlowformer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation.‚Äù In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1599‚Äì1610.\n\n\nTeed, Zachary, and Jia Deng. 2020. ‚ÄúRAFT: Recurrent All-Pairs Field Transforms for Optical Flow.‚Äù In Computer Vision ‚Äì ECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part II, 402‚Äì19. Berlin, Heidelberg: Springer-Verlag. https://doi.org/10.1007/978-3-030-58536-5_24."
  },
  {
    "objectID": "blog/2025-04-19/index.html#main-components",
    "href": "blog/2025-04-19/index.html#main-components",
    "title": "Visual guide to Optical Flow üåä",
    "section": "Main Components",
    "text": "Main Components\nI‚Äôm going to walk through the main components of computing optical flow by taking a two frame example. Hover over the image below to visualize the movement from frame1 to frame2.\n\n\n\n\n\n\nInteractive Demonstration\n\n\n\n\n\nHover over the image visualization below to see:\n\nTop: Frame1\nRevealed: Frame2\n\nNotice how:\n\nUFO moves to the right\nChewbaka rotates\nTotoro moves down\n\n\n\n\n\n \n\n\n\n\nVisualization\nThe most widely used optical flow visual representation, standardized by Baker et al. (2007), employs a color-encoded flow field where:\n\nHue (color) represents the direction of motion (0-360 degrees mapped to the color wheel)\nSaturation/Brightness represents the magnitude of displacement (brighter = faster motion)\n\nMagnitude scales from white (no motion) to maximum saturation at the flow normalization limit. A reference implementation is available in flow_viz.py.\n\nInteractive Flow Explorer\nExplore the flow field below - hover to see how pixel displacements map to actual motion vectors:\n\n\n\n\nFigure 1: Flow field color coding\n\n\n\n\n\nHover over the flow field to see displacements (dx, dy).\n\n\n\nWhat you‚Äôre seeing:\n\nThe arrow shows the displacement from the image center to your cursor position\nLonger arrows = greater motion magnitude\nArrow direction matches the color wheel convention\n\nBelow is the color-coded optical flow estimated between the two example frames shown earlier‚Äîcapturing how each pixel moved from frame1 to frame2.\n\n\n\n\nFigure 2: Visualization of flow from frame1 to frame2\n\n\n\nFor precise analysis, we often use quiver plots that explicitly show motion vectors:\n\n\n\n\nFigure 3: Arrow field projected on frame1\n\n\n\n\n\n\n\nWarping\nUsing the forward optical flow F‚ÇÅ‚ÇÇ, we can ‚Äúreverse-project‚Äù or warp the second frame (I‚ÇÇ) back in time to approximate the first frame (I‚ÇÅ). This operation effectively maps pixels from their new positions back to their original locations, using the displacement vectors in the flow field.\nThe diagram below illustrates this concept: given two images and the forward optical flow between them, we can use backward warping to reconstruct the earlier frame.\n\n\n\n\n\nflowchart LR\n  A[Image 1] --&gt; C(Forward Optical Flow)\n  B[Image 2] --&gt; C(Forward Optical Flow)\n  C(Forward Optical Flow) --&gt; D(Backward Warp)\n  B[Image 2] --&gt; D(Backward Warp)\n  D(Backward Warp) --&gt; E[Warped Image 1] \n\n\n\n\n\n\nTo understand this intuitively, let‚Äôs walk through a simplified 2√ó2 example where you can easily trace pixel displacements and their impact on the warped image.\n\n\n\n\nFigure 4: A toy example showing how forward optical flow is computed in the x and y direction\n\n\n\nIn practice, warping is implemented using a resampling operation‚Äîspecifically grid_sample in PyTorch‚Äîwhich shifts pixels according to the flow and interpolates the values at non-integer locations.\n\n\n\n\n\n\nImage Warping Code (Click to expand)\n\n\n\n\n\n\ndef image_warp(image, flow):\n    \"\"\"\n    Warps an image using optical flow with bilinear interpolation.\n    \n    Args:\n        image (torch.Tensor): Input image tensor of shape [B, C, H, W]\n        flow (torch.Tensor): Optical flow tensor of shape [B, 2, H, W] \n                    where:\n                        flow[:,0,...] is horizontal (x) displacement in pixels\n                        flow[:,1,...] is vertical (y) displacement in pixels\n    Returns:\n        torch.Tensor: Warped image [B, C, H, W]\n    \"\"\"\n    \n    B, C, H, W = image.size()\n\n    # Create base grid coordinates\n    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n    \n    # Reshape to [B, 1, H, W] for batch processing\n    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n    \n    # Combine x and y coordinates into a single grid [B, 2, H, W]\n    grid = torch.cat((xx, yy), 1).float().to(device)\n\n    # Apply optical flow displacement (in pixel coordinates)\n    vgrid = grid + flow\n\n    # Normalize grid coordinates to [-1, 1] range (required by grid_sample)\n    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n\n    #[B, H, W, 2]\n    vgrid = vgrid.permute(0, 2, 3, 1)\n\n    # Sample image using the flow-warped grid with bilinear interpolation\n    output = F.grid_sample(image, vgrid, mode='bilinear', padding_mode='zeros')\n\n    return output\n\n\n\n\nUsing the flow from our earlier example, we warp frame2 back in time to approximate frame1.\n\n\n\n\nFigure 5: Backward warped frame2\n\n\n\nIf you look closely at the warped image, you‚Äôll notice artifacts‚Äîespecially around object boundaries where motion occurred. For example, observe the region around the UFO. Why does this happen, even when the flow is accurate? I‚Äôll explain this phenomenon in the next section.\n\n\n\nHandling Occlusions\nWhen computing photometric loss, the brightness constancy assumption breaks down in occluded regions or when objects move out of view. To avoid penalizing these areas incorrectly, we need to estimate occlusions and mask them out.\n\nWhy Occlusion Matters\nOcclusions create two fundamental challenges for optical flow:\n\nDisappearing pixels: When objects leave the frame or become hidden\nNewly visible areas: When background becomes exposed\n\nThese violate the brightness constancy assumption, leading to:\n\nFalse matches in occluded regions\nInflated errors from unmatchable pixels\nArtifacts in warped images\n\n\n\n\n\n\n\nInteractive Demonstration\n\n\n\n\n\nHover over the error visualization below to see:\n\nTop: Raw photometric error\nRevealed: Error after occlusion masking\n\nNotice how masking:\n\nRemoves noise in disoccluded regions (bright areas)\nPreserves sharp boundaries\nFocuses error on reliable pixels\n\n\n\n\n\n \n\n\nKey Benefits of Occlusion Masking:\n\n‚úÇÔ∏è Removes ‚Äúghosting‚Äù effects at motion boundaries\nüéØ Focuses optimization on reliable pixels\nüìâ Reduces error propagation in iterative refinement\n\n\n\nImplementation Approaches\nHere are two common approaches:\n\n\n1. Bidirectional\n\n\n\n\n\n\nBidirectional Occlusion Mask Code (Click to expand)\n\n\n\n\n\ndef get_occu_mask_bidirection(flow12, flow21, scale=0.01, bias=0.5):\n    flow21_warped = flow_warp(flow21, flow12, pad='zeros')\n    flow12_diff = flow12 + flow21_warped\n    mag = (flow12 * flow12).sum(1, keepdim=True) + \\\n          (flow21_warped * flow21_warped).sum(1, keepdim=True)\n    occ_thresh = scale * mag + bias\n    occ = (flow12_diff * flow12_diff).sum(1, keepdim=True) &gt; occ_thresh\n    return occ.float()\n\n\n\n\n\n\n\nFigure 6: Occlusion mask generated using bidirectional consistency check on the original example.\n\n\n\nKey Idea:\n\nWarps the backward flow to forward flow coordinates\nChecks consistency between forward flow and warped backward flow\n\nAreas with large inconsistencies are marked as occluded\nAdaptive threshold combines absolute and flow-magnitude-dependent terms\n\nWhen to Use:\n\nWhen you have computed both forward and backward flows\nFor more accurate occlusion detection in textured regions\n\n\n\n\n2. Backward\n\n\n\n\n\n\nBackward Occlusion Mask Code (Click to expand)\n\n\n\n\n\ndef get_occu_mask_bidirection(flow12, flow21, scale=0.01, bias=0.5):\n    flow21_warped = flow_warp(flow21, flow12, pad='zeros')\n    flow12_diff = flow12 + flow21_warped\n    mag = (flow12 * flow12).sum(1, keepdim=True) + \\\n          (flow21_warped * flow21_warped).sum(1, keepdim=True)\n    occ_thresh = scale * mag + bias\n    occ = (flow12_diff * flow12_diff).sum(1, keepdim=True) &gt; occ_thresh\n    return occ.float()\n\n\n\n\n\n\n\nFigure 7: Occlusion mask generated using only the backward flow.\n\n\n\nKey Idea:\n\nUses only backward flow (t+1‚Üít)\nCombines photometric error and flow magnitude\nMarks pixels as occluded if\nHigh warping error\nSignificant motion magnitude\n\nWhen to Use:\n\nWhen only backward flow is available\n\nFor faster computation with slightly less accuracy\nIn low-texture regions where bidirectional checks may fail\n\n\n\n\n\n\nMetrics\n\nL1\nThe L1 loss metric, also known as Mean Absolute Error (MAE), computes the absolute difference between the predicted and actual values. It is often used for its simplicity and robustness against outliers.\n\n\n\n\nFigure 8: L1 error visualization with occluded regions highlighted.\n\n\n\nMathematical Definition:\n\\[\nL1 = \\frac{1}{N} \\sum_{i=1}^{N} |I_1(x_i) - I_2(x_i)|\n\\]\nWhere:\n\nI_1(x_i) and I_2(x_i) are the pixel values at position x_i in images I_1 and I_2, respectively.\nN is the number of pixels.\n\nKey Characteristics:\n\nThe L1 loss is linear and easy to compute, but it does not account for the structure of the image, leading to a loss in visual similarity.\n\n\n\n\nCharbonnier\nCharbonnier loss is a smooth approximation to L1 loss that helps in preventing gradients from vanishing when dealing with small differences. It is often used for optimization tasks where the learning needs to be more stable.\n\n\n\n\nFigure 9: Charbonnier error visualization with occluded regions highlighted. (alpha=0.45)\n\n\n\nMathematical Definition:\n\\[\nL_{\\text{Charbonnier}} = \\frac{1}{N} \\sum_{i=1}^{N} \\sqrt{(I_1(x_i) - I_2(x_i))^2 + \\epsilon^2}\n\\]\nWhere:\n\n\\(\\epsilon\\) is a small constant (e.g., \\(10^{-3}\\)) to prevent division by zero.\n\\(I_1(x_i)\\) and \\(I_2(x_i)\\) are the pixel values at position \\(x_i\\) in images \\(I_1\\) and \\(I_2\\), respectively.\n\\(N\\) is the number of pixels.\n\nKey Characteristics:\n\nIt reduces the impact of outliers by smoothing the loss function, making it more stable than L1 loss during training.\n\n\n\n\nSSIM\nThe SSIM metric is designed to measure the perceptual similarity between two images by considering luminance, contrast, and structure. Unlike pixel-wise metrics (like L1), SSIM incorporates the local patterns of pixel intensities, capturing more perceptual information.\nMathematical Definition:\n\\[\n\\text{SSIM}(I_1, I_2) = \\frac{(2\\mu_1\\mu_2 + C_1)(2\\sigma_{12} + C_2)}{(\\mu_1^2 + \\mu_2^2 + C_1)(\\sigma_1^2 + \\sigma_2^2 + C_2)}\n\\]\nWhere:\n\n\\(\\mu_1, \\mu_2\\) are the mean pixel values of images \\(I_1\\) and \\(I_2\\).\n\\(\\sigma_1^2, \\sigma_2^2\\) are the variances of the pixel values in \\(I_1\\) and \\(I_2\\), respectively.\n\\(\\sigma_{12}\\) is the covariance of the pixel values between \\(I_1\\) and \\(I_2\\).\n\\(C_1\\) and \\(C_2\\) are small constants to stabilize the division with weak denominator values (typically, \\(C_1 = (K_1L)^2 and C_2 = (K_2L)^2\\), where \\(L\\) is the dynamic range of the pixel values).\n\nKey Characteristics:\n\nSSIM is highly sensitive to structural information and human visual perception, which makes it more reliable than L1 loss for tasks like image quality assessment.\n\n\n\n\nCensus\nCensus transform is a non-parametric metric that compares local binary patterns rather than absolute pixel values. It is robust to lighting changes and small intensity variations and is especially useful in scenarios with significant noise or occlusion.\nMathematical Definition:\nLet the neighborhood of a pixel in image \\(I\\) be defined by \\(N(x_i)\\), which is a square window around the pixel \\(x_i\\). The census transform \\(C(I, x_i)\\) for a pixel \\(x_i\\) in image \\(I\\) is defined as:\n\\[\nC(I, x_i) = \\prod_{x_j \\in N(x_i)} \\mathbb{I}(I(x_i) &gt; I(x_j))\n\\]\nWhere:\n\n\\(\\mathbb{I}\\) is the indicator function, which is 1 if the condition is true, and 0 otherwise.\n\\(I(x_i)\\) is the intensity at pixel \\(x_i\\) in image \\(I\\).\n\\(I(x_j)\\) is the intensity at a neighboring pixel \\(x_j\\) in the neighborhood \\(N(x_i)\\).\n\nThe census transform compares the intensity values within a local window around a pixel and encodes the result as a binary string. The census loss is then computed as the Hamming distance between the census transforms of the corresponding pixels in two images.\nKey Characteristics:\n\nRobust to illumination changes and small intensity variations.\nNot differentiable, making it challenging for optimization via gradient-based methods, but it is very effective in low-texture or noisy regions.\n\n\n\nComparison of Photometric Loss Metrics\n\n\n\n\n\n\n\n\n\n\nMetric\nDescription\n‚úÖ Pros\n‚ùå Cons\n\n\n\n\nL1\nAbsolute difference between pixel intensities.\nSimple, robust to outliers.\nIgnores structure and texture shifts.\n\n\nCharbonnier\nSmooth, differentiable variant of L1: ‚àö(x¬≤ + Œµ¬≤).\nStable optimization; handles small errors better.\nSlightly slower than plain L1.\n\n\nSSIM\nMeasures structural similarity (luminance, contrast, structure).\nCaptures perceptual/structural differences.\nSensitive to illumination; more complex to compute.\n\n\nCensus\nCompares binary patterns of local neighborhoods, not raw intensity.\nVery robust to lighting and noise.\nHarder to optimize (non-differentiable)."
  },
  {
    "objectID": "blog/2025-04-19/index.html#draft-notes",
    "href": "blog/2025-04-19/index.html#draft-notes",
    "title": "Visual guide to Optical Flow üåä",
    "section": "",
    "text": "What are the optimizations made to compute optical flow faster and more reliable?"
  },
  {
    "objectID": "blog/2025-04-19/index.html#section",
    "href": "blog/2025-04-19/index.html#section",
    "title": "Guide to Optical Flow üåä",
    "section": "",
    "text": "def image_warp(image, flow):\n    \"\"\"\n    Warps an image using optical flow with bilinear interpolation.\n    \n    Args:\n        image (torch.Tensor): Input image tensor of shape [B, C, H, W]\n        flow (torch.Tensor): Optical flow tensor of shape [B, 2, H, W] \n                    where:\n                        flow[:,0,...] is horizontal (x) displacement in pixels\n                        flow[:,1,...] is vertical (y) displacement in pixels\n    \n    Returns:\n        torch.Tensor: Warped image [B, C, H, W]\n    \"\"\""
  },
  {
    "objectID": "blog/2025-04-19/index.html#section-1",
    "href": "blog/2025-04-19/index.html#section-1",
    "title": "Guide to Optical Flow üåä",
    "section": "",
    "text": "def image_warp(image, flow):\n    \"\"\"\n    Warps an image using optical flow with bilinear interpolation.\n    \n    Args:\n        image (torch.Tensor): Input image tensor of shape [B, C, H, W]\n        flow (torch.Tensor): Optical flow tensor of shape [B, 2, H, W] \n                    where:\n                        flow[:,0,...] is horizontal (x) displacement in pixels\n                        flow[:,1,...] is vertical (y) displacement in pixels\n    \n    Returns:\n        torch.Tensor: Warped image [B, C, H, W]\n    \"\"\"\n    \n    B, C, H, W = image.size()\n\n    # Create base grid coordinates\n    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)\n    \n    # Reshape to [B, 1, H, W] for batch processing\n    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n    \n    # Combine x and y coordinates into a single grid [B, 2, H, W]\n    grid = torch.cat((xx, yy), 1).float().to(device)\n\n    # Apply optical flow displacement (in pixel coordinates)\n    vgrid = grid + flow\n\n    # Normalize grid coordinates to [-1, 1] range (required by grid_sample)\n    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0\n    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0\n\n    #[B, H, W, 2]\n    vgrid = vgrid.permute(0, 2, 3, 1)\n\n    # Sample image using the flow-warped grid with bilinear interpolation\n    output = F.grid_sample(image, vgrid, mode='bilinear', padding_mode='zeros')\n\n    return output\nShow a real world example\n\nHandling Occlusions\nWhen computing photometric loss, you will observe that it is violated when pixels are occluded or moved out of view such that there are no corresponding pixels in Image2. There are multiple ways to estimate the occlusion.\nBidirectional consistency check:\nBackward Masking:\n\ndef get_occu_mask_backward(flow21, th=0.2):\n    B, _, H, W = flow21.size()\n    base_grid = mesh_grid(B, H, W).type_as(flow21)  # B2HW\n\n    corr_map = get_corresponding_map(base_grid + flow21)  # BHW\n    occu_mask = corr_map.clamp(min=0., max=1.) &lt; th\n    return occu_mask.float()\n\n\n\nMetrics"
  },
  {
    "objectID": "blog/2025-04-19/index.html#challenges",
    "href": "blog/2025-04-19/index.html#challenges",
    "title": "Visual guide to Optical Flow üåä",
    "section": "Challenges",
    "text": "Challenges\n\nHandling Occlusions\nOcclusions occur when objects move out of view or are blocked by other objects, creating areas where flow cannot be reliably estimated. These regions lead to inaccuracies in the flow field, which can complicate tasks like video synthesis or scene reconstruction.\n\n\nLarge Motion Estimation\nEstimating optical flow for large displacements is a challenge because large motions may result in pixel mismatches or erroneous vectors. When objects move quickly across the scene, the flow field becomes more difficult to compute accurately, often leading to significant errors.\n\n\nIllumination Changes\nThe brightness constancy assumption, a core principle of many optical flow algorithms, can be violated when there are changes in lighting conditions, shadows, or reflections between consecutive frames. This makes it challenging to maintain accurate flow estimation under varying illumination.\n\n\nNon-Rigid Motion\nNon-rigid motion refers to the deformation of objects, such as a person walking or a flag flapping in the wind. Unlike rigid motion, non-rigid motion does not follow predictable patterns, which makes optical flow estimation much more complex."
  },
  {
    "objectID": "blog/2025-04-19/index.html#coarse-to-fine-optimization",
    "href": "blog/2025-04-19/index.html#coarse-to-fine-optimization",
    "title": "Guide to Optical Flow üåä",
    "section": "Coarse-to-Fine Optimization",
    "text": "Coarse-to-Fine Optimization"
  },
  {
    "objectID": "blog/2025-04-19/index.html#sota-2024",
    "href": "blog/2025-04-19/index.html#sota-2024",
    "title": "Visual guide to Optical Flow üåä",
    "section": "SOTA (2024)",
    "text": "SOTA (2024)\n\nRAFT Recurrent All-Pairs Field Transforms\nBuilds dense 4D all-pairs correlation volumes by matching every pixel with every other and employs a lightweight recurrent GRU-style updater to iteratively refine the flow field (Teed and Deng 2020).\n\n\n\n\nRAFT architecture showing the correlation volume computation and iterative updates\n\n\n\n\n\nFlowFormer++\nImproves optical flow estimation by introducing a hierarchical Transformer-based architecture with cost-volume encoding and decoupled update modules, enabling long-range and fine-grained motion modeling (Shi et al. 2023).\n\n\n\n\nFlowFormer++ architecture showing the transformer-based matching and cost volume encoding"
  },
  {
    "objectID": "blog/2025-04-19/index.html#mathematical-intuition",
    "href": "blog/2025-04-19/index.html#mathematical-intuition",
    "title": "Visual guide to Optical Flow üåä",
    "section": "Mathematical Intuition",
    "text": "Mathematical Intuition\nThe foundational equation of optical flow is based on the brightness constancy assumption, which states that the intensity of a pixel does not change between consecutive frames. This assumption leads to the well-known optical flow constraint equation:\n\\[\nI_x u + I_y v + I_t = 0\n\\]\nWhere:\n\n\\(I_x\\) and \\(I_y\\) are the partial derivatives\n\\(I_t\\) is the partial derivative of the image intensity with respect to time (\\(t\\)).\n\\(u\\) and \\(v\\) are the components of the optical flow vector (horizontal, vertical).\n\nThe equation above implies that for each pixel, the rate of change of intensity over time can be attributed to the motion of the pixel in the image plane. This is the optical flow constraint.\n\n\nHorn and Schunck‚Äôs Optical Flow Model\n(Horn and Schunck 1981) introduced a regularized version of the optical flow problem to address the ill-posed optical flow equation. Their model adds a smoothness assumption, which assumes that neighboring pixels have similar motion.\nThe formulation they proposed can be written as:\n\\[\n\\min_{u,v} \\int \\left[ (I_x u + I_y v + I_t)^2 + \\alpha^2 (I_x^2 + I_y^2)(u_x^2 + u_y^2 + v_x^2 + v_y^2) \\right] dx dy\n\\]\nWhere:\n\nThe first term is the data term, which enforces the optical flow constraint.\nThe second term is the smoothness term, which enforces spatial smoothness of the flow field. It penalizes large variations in the flow between neighboring pixels.\n\\(\\alpha\\) is a regularization parameter that controls the trade-off between the data term and the smoothness term.\n\nHere, \\(u_x\\) and \\(u_y\\) represent the spatial derivatives of \\(u\\) (horizontal), and \\(v_x\\) and \\(v_y\\) represent the spatial derivatives of \\(v\\) (vertical). The smoothness term ensures that the flow field does not have sharp discontinuities, which helps in obtaining a physically plausible solution.\n\nKey Assumptions of the Horn-Schunck Method:\n\nBrightness Constancy: The pixel intensity does not change over time for any given object in the scene.\nSmoothness: The flow is smooth across the image, meaning that neighboring pixels move in similar ways.\nGlobal Regularization: The optical flow is computed globally across the entire image, meaning that all pixels are influenced by the flow of neighboring pixels through the regularization term.\n\nThis model led to a well-known global method for optical flow computation, which involves solving the system of equations derived from the above functional using iterative techniques, such as gradient descent or other optimization algorithms."
  }
]